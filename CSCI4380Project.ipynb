{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e6860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "# Visualization (optional; only used in EDA snippets)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing / Modeling\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# statsmodels (for Beta regression if available)\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.genmod.families import Beta as SM_BetaFamily  # may not exist in older versions\n",
    "    from statsmodels.genmod.families.links import logit as sm_logit\n",
    "    STATS_BETA_AVAILABLE = True\n",
    "except Exception:\n",
    "    STATS_BETA_AVAILABLE = False\n",
    "\n",
    "from scipy.special import expit, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46dfd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "ensure_dir(\"outputs\")\n",
    "\n",
    "def safe_logit(y: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Apply logit to y in (0,1) with clipping for numerical stability.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = np.clip(y, eps, 1 - eps)\n",
    "    return logit(y)\n",
    "\n",
    "def safe_expit(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Inverse of logit (sigmoid).\"\"\"\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return expit(z)\n",
    "\n",
    "def normalize_to_0_1(y: pd.Series, eps: float = 1e-6) -> pd.Series:\n",
    "    \"\"\"Normalize a positive, bounded target (e.g., PER) to (0,1).\n",
    "    Uses min-max with tiny epsilon padding to avoid exact 0/1 values for Beta modeling.\"\"\"\n",
    "    y = y.astype(float)\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    denom = (ymax - ymin) + 2 * eps\n",
    "    return (y - ymin + eps) / denom\n",
    "\n",
    "def per40(x: pd.Series, minutes: pd.Series) -> pd.Series:\n",
    "    \"\"\"Per-40 minutes rate: statistic per 40 minutes of play.\"\"\"\n",
    "    return (x / minutes.replace(0, np.nan)) * 40.0\n",
    "\n",
    "def per100_possessions(x: pd.Series, team_possessions: pd.Series) -> pd.Series:\n",
    "    \"\"\"Per-100 possessions rate.\"\"\"\n",
    "    return (x / team_possessions.replace(0, np.nan)) * 100.0\n",
    "\n",
    "def true_shooting_percentage(points: pd.Series, fga: pd.Series, fta: pd.Series) -> pd.Series:\n",
    "    \"\"\"TS% = PTS / (2 * (FGA + 0.44*FTA))\"\"\"\n",
    "    denom = 2.0 * (fga + 0.44 * fta)\n",
    "    return points / denom.replace(0, np.nan)\n",
    "\n",
    "def effective_fg_percentage(fgm: pd.Series, three_pm: pd.Series, fga: pd.Series) -> pd.Series:\n",
    "    \"\"\"eFG% = (FGM + 0.5*3PM) / FGA\"\"\"\n",
    "    return (fgm + 0.5 * three_pm) / fga.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e8158b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "college_path = kagglehub.dataset_download(\"adityak2003/college-basketball-players-20092021\")\n",
    "combine_path = kagglehub.dataset_download(\"marcusfern/nba-draft-combine\")\n",
    "rookie_path = kagglehub.dataset_download(\"thedevastator/nba-rookies-performance-statistics-and-minutes-p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fa5b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Ethan\\.cache\\kagglehub\\datasets\\adityak2003\\college-basketball-players-20092021\\versions\\5\n",
      "Path to dataset files: C:\\Users\\Ethan\\.cache\\kagglehub\\datasets\\marcusfern\\nba-draft-combine\\versions\\15\n",
      "Path to dataset files: C:\\Users\\Ethan\\.cache\\kagglehub\\datasets\\thedevastator\\nba-rookies-performance-statistics-and-minutes-p\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "print(\"Path to dataset files:\", college_path)\n",
    "print(\"Path to dataset files:\", combine_path)\n",
    "print(\"Path to dataset files:\", rookie_path)\n",
    "\n",
    "DATA_PATHS = {\n",
    "    # College-level stats (one row per player-season or player-career; include Minutes, team possessions if available)\n",
    "    \"college_csv\": os.path.join(college_path, \"CollegeBasketballPlayers2009-2021.csv\"),\n",
    "    # NBA combine measurements (one row per player-year; columns like Height, Wingspan, Weight, Vertical, etc.)\n",
    "    \"combine_csv\": os.path.join(combine_path, \"Draft Combine - Kaggle.csv\"),\n",
    "    # Rookie PER (one row per player; include rookie PER and rookie season/draft year), e.g., from Basketball-Reference\n",
    "    \"rookie_csv\": os.path.join(rookie_path, \"NBA Rookies by Year.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0c2d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_name(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize player names across datasets:\n",
    "    - Removes punctuation and suffixes\n",
    "    - Handles 'Last, First' â†’ 'First Last'\n",
    "    - Collapses spaces and uppercases\n",
    "    \"\"\"\n",
    "    if s is None or not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "\n",
    "    # If formatted like \"Smith, John\", flip to \"John Smith\"\n",
    "    if \",\" in s:\n",
    "        parts = [p.strip() for p in s.split(\",\")]\n",
    "        if len(parts) == 2:\n",
    "            s = f\"{parts[1]} {parts[0]}\"\n",
    "\n",
    "    # Remove punctuation/apostrophes\n",
    "    s = re.sub(r\"[.\\u2019'`]\", \"\", s)\n",
    "    s = re.sub(r\"\\b(JR|SR|II|III|IV)\\b\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    return s.upper().strip()\n",
    "\n",
    "def percent_to_unit(df: pd.DataFrame, cols):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns and pd.api.types.is_numeric_dtype(df[c]):\n",
    "            s = df[c]\n",
    "            # if most values > 1, assume 0..100 and convert\n",
    "            if s.dropna().gt(1.5).mean() > 0.5:\n",
    "                df[c] = s / 100.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0d68e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_college(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    rename_map = {\n",
    "        \"player_name\": \"player\",\n",
    "        \"team\": \"school\",          \n",
    "        \"year\": \"season\",    \n",
    "        \"\": \"position\",      \n",
    "        \"GP\": \"gp\",\n",
    "        \"mp\": \"mpg\",           \n",
    "\n",
    "        \"FTM\": \"ftm\",\n",
    "        \"FTA\": \"fta\",\n",
    "        \"FT_per\": \"ft_pct\",\n",
    "        \"twoPM\": \"twopm\",\n",
    "        \"twoPA\": \"twopa\",\n",
    "        \"twoP_per\": \"twop_pct\",\n",
    "        \"TPM\": \"fg3m\",             \n",
    "        \"TPA\": \"fg3a\",             \n",
    "        \"TP_per\": \"fg3_pct\",\n",
    "\n",
    "        \"pts\": \"pts_pg\",\n",
    "        \"ast\": \"ast_pg\",\n",
    "        \"treb\": \"trb_pg\",\n",
    "        \"stl\": \"stl_pg\",\n",
    "        \"blk\": \"blk_pg\",\n",
    "\n",
    "        \"eFG\": \"efg_pct_given\",\n",
    "        \"TS_per\": \"ts_pct_given\",\n",
    "\n",
    "        \"ORB_per\": \"orb_pct\",\n",
    "        \"DRB_per\": \"drb_pct\",\n",
    "        \"AST_per\": \"ast_pct\",\n",
    "        \"TO_per\": \"tov_pct\",\n",
    "        \"ftr\": \"ftr\",\n",
    "\n",
    "        \"Ortg\": \"ortg\",\n",
    "        \"Drtg\": \"drtg\",\n",
    "    \n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "    df = df[df[\"pick\"].notna()]\n",
    "\n",
    "    if {\"gp\", \"mpg\"}.issubset(df.columns):\n",
    "        df[\"minutes\"] = df[\"gp\"] * df[\"mpg\"]\n",
    "    else:\n",
    "        df[\"minutes\"] = np.nan\n",
    "\n",
    "    \n",
    "    df[\"fgm\"] = df[\"twopm\"].fillna(0) + df[\"fg3m\"].fillna(0)\n",
    "    \n",
    "    df[\"fga\"] = df[\"twopa\"].fillna(0) + df[\"fg3a\"].fillna(0)\n",
    "\n",
    "    df = df.sort_values([\"player\", \"season\"]).groupby(\"player\", as_index=False).tail(1).rename(columns={\"season\": \"draft_year\"})\n",
    "    \n",
    "    df = df.drop_duplicates(subset=[c for c in [\"player\", \"season\", \"school\"] if c in df.columns])\n",
    "    return df\n",
    "\n",
    "def load_combine(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load NBA Combine measurements.\n",
    "\n",
    "    Expected columns (examples):\n",
    "        - player\n",
    "        - draft_year (int)\n",
    "        - height_w_shoes_in, wingspan_in, standing_reach_in, weight_lbs\n",
    "        - body_fat_pct, bench_reps, vert_no_step_in, vert_max_in, lane_agility_sec, shuttle_run_sec, three_quarter_sprint_sec\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    rename_map = {\n",
    "        \"PLAYER\": \"player\",\n",
    "        \"YEAR\": \"draft_year\",\n",
    "        \"POS\": \"position\",\n",
    "\n",
    "        # Measurements\n",
    "        \"HGT\": \"height_w_shoes_in\",    \n",
    "        \"WGT\": \"weight_lbs\",\n",
    "        \"BMI\": \"bmi\",\n",
    "        \"BF\": \"body_fat_pct\",\n",
    "        \"WNGSPN\": \"wingspan_in\",\n",
    "        \"STNDRCH\": \"standing_reach_in\",\n",
    "        \"HANDL\": \"hand_length_in\",\n",
    "        \"HANDW\": \"hand_width_in\",\n",
    "\n",
    "        # Jump results\n",
    "        \"STNDVERT\": \"vert_no_step_in\",\n",
    "        \"LPVERT\": \"vert_max_in\",\n",
    "        \"PBHGT\": \"standing_reach_plus_vert_in\",       \n",
    "        \"PDHGT\": \"standing_reach_plus_max_vert_in\",   \n",
    "\n",
    "        # Agility & speed\n",
    "        \"LANE\": \"lane_agility_sec\",\n",
    "        \"SHUTTLE\": \"shuttle_run_sec\",\n",
    "        \"SPRINT\": \"three_quarter_sprint_sec\",\n",
    "\n",
    "        # Strength\n",
    "        \"BENCH\": \"bench_reps\",\n",
    "\n",
    "        # Extras\n",
    "        \"BAR\": \"bar\",\n",
    "        \"PAN\": \"pan\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "    numeric_like = [\n",
    "        \"draft_year\", \"height_w_shoes_in\", \"weight_lbs\", \"bmi\", \"body_fat_pct\",\n",
    "        \"wingspan_in\", \"standing_reach_in\", \"hand_length_in\", \"hand_width_in\",\n",
    "        \"vert_no_step_in\", \"vert_max_in\",\n",
    "        \"standing_reach_plus_vert_in\", \"standing_reach_plus_max_vert_in\",\n",
    "        \"lane_agility_sec\", \"shuttle_run_sec\", \"three_quarter_sprint_sec\",\n",
    "        \"bench_reps\", \"bar\", \"pan\"\n",
    "    ]\n",
    "    for c in numeric_like:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.drop_duplicates(subset=[c for c in [\"player\", \"draft_year\"] if c in df.columns])\n",
    "    return df\n",
    "\n",
    "def load_rookie(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Drop unnamed/index\n",
    "    unnamed = [c for c in df.columns if not str(c).strip() or str(c).lower().startswith(\"unnamed\") or str(c).lower()==\"index\"]\n",
    "    if unnamed: \n",
    "        df = df.drop(columns=unnamed)\n",
    "\n",
    "    rename_map = {\"Name\":\"player\",\n",
    "                  \"Year Drafted\":\"rookie_season\",\n",
    "                  \"GP\":\"gp\",\n",
    "                  \"MIN\":\"minutes\",\n",
    "                  \"PTS\":\"pts\",\n",
    "                  \"FGM\":\"fgm\",\n",
    "                  \"FGA\":\"fga\",\n",
    "                  \"FG%\":\"fg_pct\",\n",
    "                  \"3P Made\":\"fg3m\",\n",
    "                  \"3PA\":\"fg3a\",\n",
    "                  \"3P%\":\"fg3_pct\",\n",
    "                  \"FTM\":\"ftm\",\n",
    "                  \"FTA\":\"fta\",\n",
    "                  \"FT%\":\"ft_pct\",\n",
    "                  \"OREB\":\"orb\",\n",
    "                  \"DREB\":\"drb\",\n",
    "                  \"REB\":\"trb\",\n",
    "                  \"AST\":\"ast\",\n",
    "                  \"STL\":\"stl\",\n",
    "                  \"BLK\":\"blk\",\n",
    "                  \"TOV\":\"tov\",\n",
    "                  \"EFF\":\"rookie_per\"\n",
    "            }\n",
    "    df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
    "    # Types\n",
    "    for c in [\"rookie_season\",\"gp\",\"minutes\",\"pts\",\"fgm\",\"fga\",\"fg_pct\",\"fg3m\",\"fg3a\",\"fg3_pct\",\"ftm\",\"fta\",\"ft_pct\",\"orb\",\"drb\",\"trb\",\"ast\",\"stl\",\"blk\",\"tov\",\"rookie_per\"]:\n",
    "        if c in df.columns: \n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[\"draft_year\"] = df[\"rookie_season\"]\n",
    "    if {\"player\",\"rookie_season\"}.issubset(df.columns): \n",
    "        df = df.drop_duplicates(subset=[\"player\",\"rookie_season\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70344ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rate_features(college_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = college_df.copy()\n",
    "\n",
    "    # per-40s from totals\n",
    "    if {\"pts\", \"minutes\"}.issubset(df.columns):\n",
    "        df[\"pts_per40\"] = per40(df[\"pts\"], df[\"minutes\"])\n",
    "    if {\"ast\", \"minutes\"}.issubset(df.columns):\n",
    "        df[\"ast_per40\"] = per40(df[\"ast\"], df[\"minutes\"])\n",
    "    if {\"trb\", \"minutes\"}.issubset(df.columns):\n",
    "        df[\"trb_per40\"] = per40(df[\"trb\"], df[\"minutes\"])\n",
    "    if {\"stl\", \"minutes\"}.issubset(df.columns):\n",
    "        df[\"stl_per40\"] = per40(df[\"stl\"], df[\"minutes\"])\n",
    "    if {\"blk\", \"minutes\"}.issubset(df.columns):\n",
    "        df[\"blk_per40\"] = per40(df[\"blk\"], df[\"minutes\"])\n",
    "    if {\"tov\", \"minutes\"}.issubset(df.columns):\n",
    "        df[\"tov_per40\"] = per40(df.get(\"tov\", np.nan), df[\"minutes\"])\n",
    "\n",
    "    # efficiency recompute (fallback to given if components are missing)\n",
    "    if {\"fgm\", \"fg3m\", \"fga\"}.issubset(df.columns) and df[\"fga\"].notna().any():\n",
    "        df[\"efg_pct\"] = effective_fg_percentage(df[\"fgm\"], df[\"fg3m\"], df[\"fga\"])\n",
    "    elif \"efg_pct_given\" in df.columns:\n",
    "        df[\"efg_pct\"] = df[\"efg_pct_given\"]\n",
    "\n",
    "    if {\"pts\", \"fga\", \"fta\"}.issubset(df.columns) and (df[\"fga\"].notna().any() or df[\"fta\"].notna().any()):\n",
    "        df[\"ts_pct\"] = true_shooting_percentage(df[\"pts\"], df[\"fga\"], df[\"fta\"])\n",
    "    elif \"ts_pct_given\" in df.columns:\n",
    "        df[\"ts_pct\"] = df[\"ts_pct_given\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def select_model_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    numeric_candidates = [\n",
    "        # college per-40/efficiency\n",
    "        \"pts_per40\", \"ast_per40\", \"trb_per40\", \"stl_per40\", \"blk_per40\", \"tov_per40\",\n",
    "        \"efg_pct\", \"ts_pct\",\n",
    "        # combine\n",
    "        \"height_w_shoes_in\", \"wingspan_in\", \"standing_reach_in\", \"weight_lbs\",\n",
    "        \"body_fat_pct\", \"bench_reps\", \"vert_no_step_in\", \"vert_max_in\",\n",
    "        \"lane_agility_sec\", \"shuttle_run_sec\", \"three_quarter_sprint_sec\",\n",
    "    ]\n",
    "    categorical_candidates = [\"position\"] \n",
    "    numeric_cols = [c for c in numeric_candidates if c in df.columns]\n",
    "    categorical_cols = [c for c in categorical_candidates if c in df.columns]\n",
    "    return df, numeric_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "046432e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_college_combine_rookie(college_df: pd.DataFrame, combine_df: pd.DataFrame, rookie_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfc = college_df.copy()\n",
    "    \n",
    "    merged = dfc.merge(combine_df, on=[\"player\",\"draft_year\"], how=\"left\")\n",
    "    merged = merged.merge(rookie_df[[\"player\",\"draft_year\",\"rookie_per\"]], on=[\"player\",\"draft_year\"], how=\"inner\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2560a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(numeric_cols: List[str], categorical_cols: List[str]) -> ColumnTransformer:\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipeline, numeric_cols),\n",
    "            (\"cat\", categorical_pipeline, categorical_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def train_val_split(df: pd.DataFrame, feature_cols: List[str], target_col: str = \"rookie_per\", test_size: float = 0.2):\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c9b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linear_regression(df_model: pd.DataFrame, numeric_cols: List[str], categorical_cols: List[str], cv_splits: int = 5) -> dict:\n",
    "    pre = build_preprocessor(numeric_cols, categorical_cols)\n",
    "    model = Pipeline([(\"preprocess\", pre), (\"model\", LinearRegression())])\n",
    "    X = df_model[numeric_cols + categorical_cols]; y = df_model[\"rookie_per\"]\n",
    "    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    neg_mae = cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    neg_mse = cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "    r2 = cross_val_score(model, X, y, cv=kf, scoring=\"r2\")\n",
    "    return {\"MAE\": float(-neg_mae.mean()), \"RMSE\": float(np.sqrt(-neg_mse.mean())), \"R2\": float(r2.mean())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "181971aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_beta_regression(\n",
    "    df_model: pd.DataFrame,\n",
    "    numeric_cols: List[str],\n",
    "    categorical_cols: List[str],\n",
    "    cv_splits: int = 5\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate Beta Regression two ways:\n",
    "    1) Preferred: statsmodels GLM with Beta family (if available).\n",
    "    2) Fallback: logit-transform the target in (0,1) using TransformedTargetRegressor + LinearRegression.\n",
    "    \"\"\"\n",
    "    # Normalize target to (0,1)\n",
    "    y01 = normalize_to_0_1(df_model[\"rookie_per\"])\n",
    "\n",
    "    if STATS_BETA_AVAILABLE:\n",
    "        # Prepare design matrix using the same preprocessing as Linear Regression\n",
    "        preprocessor = build_preprocessor(numeric_cols, categorical_cols)\n",
    "        X = df_model[numeric_cols + categorical_cols]\n",
    "        X_design = preprocessor.fit_transform(X)\n",
    "        # Add constant for intercept\n",
    "        X_design = sm.add_constant(X_design, has_constant=\"add\")\n",
    "\n",
    "        # Fit GLM Beta\n",
    "        model = sm.GLM(y01, X_design, family=SM_BetaFamily(sm_logit()))\n",
    "        res = model.fit()\n",
    "        # In-sample predictions for quick baseline metrics (could implement CV with refits if desired)\n",
    "        preds = res.predict(X_design)\n",
    "        # Map back from (0,1) to original PER scale for error metrics\n",
    "        per_min, per_max = df_model[\"rookie_per\"].min(), df_model[\"rookie_per\"].max()\n",
    "        eps = 1e-6\n",
    "        preds_per = (preds * ((per_max - per_min) + 2*eps)) + (per_min - eps)\n",
    "\n",
    "        mae = mean_absolute_error(df_model[\"rookie_per\"], preds_per)\n",
    "        rmse = math.sqrt(mean_squared_error(df_model[\"rookie_per\"], preds_per))\n",
    "        r2 = r2_score(df_model[\"rookie_per\"], preds_per)\n",
    "        return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"used\": \"statsmodels.GLM(Beta)\"}\n",
    "    else:\n",
    "        # Fallback: approximate beta regression with logit-transformed target\n",
    "        preprocessor = build_preprocessor(numeric_cols, categorical_cols)\n",
    "        base = LinearRegression()\n",
    "\n",
    "        # target transform y->logit(y01), inverse logit back to (0,1)\n",
    "        y01_series = y01.copy()\n",
    "\n",
    "        def tfunc(y):\n",
    "            return safe_logit(y)\n",
    "\n",
    "        def inv_tfunc(z):\n",
    "            return safe_expit(z)\n",
    "\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"tt\", TransformedTargetRegressor(regressor=base, func=tfunc, inverse_func=inv_tfunc))\n",
    "        ])\n",
    "\n",
    "        X = df_model[numeric_cols + categorical_cols]\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "        # CV on normalized (0,1) target, then convert back to PER scale for reporting\n",
    "        preds_all = []\n",
    "        y_all = []\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y01_series.iloc[train_idx], y01_series.iloc[test_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            p = model.predict(X_test)  # in (0,1)\n",
    "            preds_all.append(p)\n",
    "            y_all.append(y_test.values)\n",
    "\n",
    "        preds_all = np.concatenate(preds_all)\n",
    "        y_all = np.concatenate(y_all)\n",
    "\n",
    "        # Map (0,1) back to PER scale based on global min/max\n",
    "        per_min, per_max = df_model[\"rookie_per\"].min(), df_model[\"rookie_per\"].max()\n",
    "        eps = 1e-6\n",
    "        preds_per = (preds_all * ((per_max - per_min) + 2*eps)) + (per_min - eps)\n",
    "        y_true_per = (y_all * ((per_max - per_min) + 2*eps)) + (per_min - eps)\n",
    "\n",
    "        mae = mean_absolute_error(y_true_per, preds_per)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true_per, preds_per))\n",
    "        r2 = r2_score(y_true_per, preds_per)\n",
    "        return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"used\": \"logit-TransformedTargetRegressor\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (217, 91)\n",
      "          player  draft_year\n",
      "        AJ PRICE        2009\n",
      " HASHEEM THABEET        2009\n",
      "  DAJUAN SUMMERS        2009\n",
      "DANTE CUNNINGHAM        2009\n",
      " DARREN COLLISON        2009\n",
      "          player  draft_year\n",
      "       KRIS DUNN        2016\n",
      "    CARIS LEVERT        2016\n",
      "DENZEL VALENTINE        2016\n",
      "   PASCAL SIAKAM        2016\n",
      "ISAIAH WHITEHEAD        2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_32304\\4079110041.py:2: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n",
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1214: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (rk == rk.astype(lk.dtype))[~np.isnan(rk)].all():\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data\n",
    "college = load_college(DATA_PATHS[\"college_csv\"])\n",
    "combine = load_combine(DATA_PATHS[\"combine_csv\"])\n",
    "rookie = load_rookie(DATA_PATHS[\"rookie_csv\"])\n",
    "for df in (college, combine, rookie):\n",
    "    df[\"player\"] = df[\"player\"].astype(str).map(clean_player_name)\n",
    "    \n",
    "college_fe = add_rate_features(college)\n",
    "\n",
    "# 3) Merge\n",
    "merged = merge_college_combine_rookie(college_fe, combine, rookie)\n",
    "pct_cols = [\n",
    "    \"efg_pct_given\",\"ts_pct_given\",\"ft_pct_given\",\"twop_pct_given\",\"fg3_pct_given\",\n",
    "    \"efg_pct\",\"ts_pct\",\"fg_pct\",\"fg3_pct\",\"ft_pct\"\n",
    "]\n",
    "merged = percent_to_unit(merged, pct_cols)\n",
    "print (\"Merged dataset shape:\", merged.shape)\n",
    "print(\n",
    "    merged.sort_values(\"draft_year\")[[\"player\", \"draft_year\"]]\n",
    "    .head()\n",
    "    .to_string(index=False)\n",
    ")\n",
    "print(\n",
    "    merged.sort_values(\"draft_year\")[[\"player\", \"draft_year\"]]\n",
    "    .tail()\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "# 4) Choose feature sets\n",
    "df_all, num_cols, cat_cols = select_model_features(merged)\n",
    "\n",
    "if not num_cols and not cat_cols:\n",
    "    raise ValueError(\"No feature columns were found. Check your input CSV schemas and rename maps.\")\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0a0b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (CV) -> {'MAE': 3.264776867542962, 'RMSE': 4.245037803768917, 'R2': -0.24291014442584843}\n",
      "Beta Regression (CV or in-sample per availability) -> {'MAE': 3.8675313967415086, 'RMSE': 4.978110763504027, 'R2': -0.5733143297963397, 'used': 'logit-TransformedTargetRegressor'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:210: UserWarning: The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:210: UserWarning: The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:210: UserWarning: The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:210: UserWarning: The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_target.py:210: UserWarning: The provided functions or transformer are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:210: UserWarning: The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5) Evaluate Linear Regression (baseline)\n",
    "lin_metrics = evaluate_linear_regression(df_all, num_cols, cat_cols)\n",
    "print(\"Linear Regression (CV) ->\", lin_metrics)\n",
    "\n",
    "# 6) Evaluate Beta Regression (baseline)\n",
    "beta_metrics = evaluate_beta_regression(df_all, num_cols, cat_cols)\n",
    "print(\"Beta Regression (CV or in-sample per availability) ->\", beta_metrics)\n",
    "\n",
    "# 7) Persist merged training table and a quick metrics report\n",
    "merged.to_csv(\"outputs/merged_training_table.csv\", index=False)\n",
    "pd.DataFrame([lin_metrics, beta_metrics], index=[\"linear\", \"beta\"]).to_csv(\"outputs/baseline_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
